<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>KKYY的学习笔记</title><meta name="author" content="wizard"><meta name="copyright" content="wizard"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="KKYY的学习笔记">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="KKYY的学习笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/cat.jpg">
<meta property="article:author" content="wizard">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/cat.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'KKYY的学习笔记',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-12-17 17:38:08'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/cat.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-list"></i><span> 菜单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%85%B3%E4%BA%8E"><i class="fa-fw /about/"></i><span> 0</span></a></li><li><a class="site-page child" href="/myself"><i class="fa-fw /myself/"></i><span> 1</span></a></li><li><a class="site-page child" href="/butterfly%E4%B8%BB%E9%A2%98"><i class="fa-fw https://github.com/jerryc127/hexo-theme-butterfly/"></i><span> 2</span></a></li></ul></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/palade.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">KKYY的学习笔记</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-list"></i><span> 菜单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%85%B3%E4%BA%8E"><i class="fa-fw /about/"></i><span> 0</span></a></li><li><a class="site-page child" href="/myself"><i class="fa-fw /myself/"></i><span> 1</span></a></li><li><a class="site-page child" href="/butterfly%E4%B8%BB%E9%A2%98"><i class="fa-fw https://github.com/jerryc127/hexo-theme-butterfly/"></i><span> 2</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">KKYY的学习笔记</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="/atom.xml" target="_blank" title="RSS链接"><i class="iconfont icon-rss card_icon"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=710697495&amp;website=www.oicqzone.com" target="_blank" title=""><i class="iconfont icon-QQ card_icon"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/post/15f17e01.html" title="Knowledge Distillation Meets Self-Supervision"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Knowledge Distillation Meets Self-Supervision"></a></div><div class="recent-post-info"><a class="article-title" href="/post/15f17e01.html" title="Knowledge Distillation Meets Self-Supervision">Knowledge Distillation Meets Self-Supervision</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-12-14T07:49:31.000Z" title="发表于 2022-12-14 15:49:31">2022-12-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="content">核心创新
利用自监督任务来促进从教师网络到学生网络中提取更丰富的知识
研究背景
现有的研究集中在学生应该模仿什么类型的教师网络的中间表征上。这些表示包括注意图、格矩阵、梯度、预激活和特征分布统计
虽然网络的中间表示可以提供更细粒度的信息，但这些知识媒介的一个共同特征是，它们都来自单个任务（通常是原始分类任务）
研究方法
Learning SSKD

image-20221214182510942

教师和学生都由三个组成部分组成：一个用于提取表示的主干，一个分类器用于主要任务，以及一个用于特定自我监督任务的自我监督（SS）模块。本研究选择对比预测作为SS任务，因此SS模块和c_s(·,·)由一个2层MLP和一个相似度计算模块组成。
training the teacher network
把和输入到Backbone，
教师网络的训练分为两个阶段。
第一阶段，对网络的分类损失进行训练。只更新Backbone和分类器。注意，分类损失不是在转换后的数据上计算的，因为转换后的比通常的数据增强要重得多。它的目标不是扩大训练集，而是使在视觉上不那么类似于。它使得对比更加 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/7709ffe1.html" title="Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework"></a></div><div class="recent-post-info"><a class="article-title" href="/post/7709ffe1.html" title="Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework">Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-12-11T02:48:11.000Z" title="发表于 2022-12-11 10:48:11">2022-12-11</time></span></div><div class="content">12.11-Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework
核心创新

提出了一个有效的知识蒸馏框架来提取任意预训练的GNN模型的知识，并将其注入到学生模型中，以进行更有效的预测。
将学生模型设计为参数化标签传播和基于特征的2层MLP的可训练组合,保留了基于结构/特征的先验。
学习到的学生模型可以同时利用GNN和先验知识。

研究背景

gnn中图拓扑、节点特征和投影矩阵的纠缠导致了一种复杂的预测机制，不能充分利用数据中的先验知识。在图卷积中没有被充分利用
模型的主体仍然是GNN，因此很难充分利用先验知识；
它们是单一的模型而不是框架，因此与其他先进的GNN架构不兼容；
忽略了另一个重要的先验知识，即基于特征的先验，表示节点的标签完全是由自己的特征决定的。

研究方法

学生模型被训练来模拟一个预先训练好的教师模型的软标签预测。因此，教师模型中的知识将被提取并注入到已学习的学生中。 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/515ef697.html" title="Selective-Supervised Contrastive Learning with Noisy Labels"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Selective-Supervised Contrastive Learning with Noisy Labels"></a></div><div class="recent-post-info"><a class="article-title" href="/post/515ef697.html" title="Selective-Supervised Contrastive Learning with Noisy Labels">Selective-Supervised Contrastive Learning with Noisy Labels</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-12-02T12:41:18.000Z" title="发表于 2022-12-02 20:41:18">2022-12-02</time></span></div><div class="content">&lt;img src="https://" alt="" style="width:100%" /&gt; 
</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/d81d0a77.html" title="Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning"></a></div><div class="recent-post-info"><a class="article-title" href="/post/d81d0a77.html" title="Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning">Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-27T09:19:51.000Z" title="发表于 2022-11-27 17:19:51">2022-11-27</time></span></div><div class="content">&lt;img src="https://" alt="" style="width:100%" /&gt; 
</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/ef2c54c1.html" title="Self-supervised Consensus Representation Learning for Attributed Graph"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Self-supervised Consensus Representation Learning for Attributed Graph"></a></div><div class="recent-post-info"><a class="article-title" href="/post/ef2c54c1.html" title="Self-supervised Consensus Representation Learning for Attributed Graph">Self-supervised Consensus Representation Learning for Attributed Graph</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-26T00:44:28.000Z" title="发表于 2022-11-26 08:44:28">2022-11-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="content">&lt;img src="https://" alt="" style="width:100%" /&gt; 
Self-supervised Consensus Representation Learning for Attributed Graph
核心创新

提出了一种新的自监督框架来学习属性图的共识表示。探索自监督机制在融合图的拓扑信息和节点特征信息中的作用
为了学习共识表示，提出了一个自监督模块，该模块使用嵌入在一个图中的节点来预测另一个图中同一节点的分类结果

研究背景
图结构和节点特征之间存在相关性，这两个方面的底层信息可以相互监督
基于注意力的方法可以学习合适的权重来融合特征和拓扑信息。然而需要计算每个节点的权重，这对于大规模图是低效的
如何更好的融合图结构和节点特征？
研究方法

为了表示特征空间中节点的结构，通过构建了特征图。利用余弦相似度计算相似度矩阵:

表示节点特征和节点特征的的余弦相似度。对于每个节点，选择前k个最近邻并建立边。得到构造的特征图邻接矩阵。它与G共享相同的，但具有不同的邻接矩阵。因此，拓扑图和特征图分别为和
和通 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/c7b28f25.html" title="Structure-Preserving Graph Representation Learning"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Structure-Preserving Graph Representation Learning"></a></div><div class="recent-post-info"><a class="article-title" href="/post/c7b28f25.html" title="Structure-Preserving Graph Representation Learning">Structure-Preserving Graph Representation Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-24T07:52:06.000Z" title="发表于 2022-11-24 15:52:06">2022-11-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="content">&lt;img src="https://" alt="" style="width:100%" /&gt; 
Structure-Preserving Graph Representation Learning
## 核心创新

利用kN方法构建特征图，可以保持高阶邻近性。通过GCN得到的输出嵌入和的输出嵌入。通过对比损失的局部节点级关系对它们进行了细化
最大化拓扑图和特征嵌入之间的互信息来保持全局结构信息

研究背景
充分地提取和嵌入丰富的拓扑结构和特征信息仍然是一个挑战。现有的方法大多集中于局部结构，未能充分融入全局拓扑结构。不能利用全局图的信息。
现有的对比学习方法主要探讨局部关系，而不保留结构信息。依赖于数据增强的选取，涉及节点和边的随机破坏。这可能会给原始图数据引入噪声，并降低学习表示的通用性。因此，在节点级和图级有很大的提高信息利用率的空间。
研究方法
目的是充分利用图结构和节点属性之间的潜在相关性。
不仅从原始图中获取图信息，还通过特征图来利用特征视图。
通过最大化全局级MI，从特征图视图和拓扑图视图中继承了丰富的表示信息
Feature E ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/90defc9.html" title="Self-supervised Graph-level Representation Learning with Local and Global Structure"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Self-supervised Graph-level Representation Learning with Local and Global Structure"></a></div><div class="recent-post-info"><a class="article-title" href="/post/90defc9.html" title="Self-supervised Graph-level Representation Learning with Local and Global Structure">Self-supervised Graph-level Representation Learning with Local and Global Structure</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-18T08:05:04.000Z" title="发表于 2022-11-18 16:05:04">2022-11-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="content">&lt;img src="https://cdn.jsdelivr.net/gh/zhengziyu77/blog_img/202211261437561.png" alt="" style="width:100%" /&gt; 
Self-supervised Graph-level Representation Learning with Local and Global Structure
核心创新
GraphLoG通过对齐相关图/子图的嵌入来构造一个局部光滑的潜在空间。
在此基础上，利用层次原型对图嵌入的全局结构进行了建模，并通过在线EM算法实现了数据似然性的最大化。
研究背景
现有方法只对不同图实例之间的局部结构进行建模，但没有发现全局语义结构
研究方法

image-20221126143753482

Learning Local-instance Structure of Graph Representations
利用在潜在空间中定义的相似度度量，将这个问题表述为最大化相关图/子图对的相似性，同时最小化负图对的相似性。
correlated  ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/6193c4af.html" title="Prototypical Graph Contrastive Learning"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Prototypical Graph Contrastive Learning"></a></div><div class="recent-post-info"><a class="article-title" href="/post/6193c4af.html" title="Prototypical Graph Contrastive Learning">Prototypical Graph Contrastive Learning</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-17T05:38:29.000Z" title="发表于 2022-11-17 13:38:29">2022-11-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="content">Prototypical Graph Contrastive Learning
核心创新

将语义相似的图聚类到同一组，同时鼓励同一图的不同增强之间的聚类一致性
重加权对比目标，基于负样本的原型距离进行重加权，以减轻抽样偏差问题。

研究背景
现有的方法主要集中于实例级结构相似性的建模，没有探索整个数据分布上的底层全局结构。
从整个数据分布中均匀地采样负样本的做法可能导致负样本实际上在语义上与查询样本相似；采样偏差问题。
实例对比学习（instance-wise contrastive learning）学习一个嵌入空间，它只保留每个实例周围的局部相似性，但在很大程度上忽略了整个图数据的全局语义结构
如何对结构和全局语义信息进行负对采样是图对比学习的关键
研究方法
目标是将语义上相似的图聚类到同一组中，并同时鼓励同一图的不同增强量（即相关视图）之间的聚类一致性。

image-20221118005533161

Clustering Consistency for Correlated Views
GNN encode得到, 将所有表示的聚类为K个聚 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/post/31cc5f36.html" title="Augmentation-Free Self-Supervised Learning on Graphs"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Augmentation-Free Self-Supervised Learning on Graphs"></a></div><div class="recent-post-info"><a class="article-title" href="/post/31cc5f36.html" title="Augmentation-Free Self-Supervised Learning on Graphs">Augmentation-Free Self-Supervised Learning on Graphs</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-13T14:11:14.000Z" title="发表于 2022-11-13 22:11:14">2022-11-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="content">核心创新
基于增强的对比方法依赖于增强方式的选择和超参数的选取，不通过数据增强来生成新视图，通过发现与图共享局部结构信息和全局语义的节点来生成图的另一种视图

主要创新点在于如何构建正样本对（不通过数据增强的方式），然后使用了BYOL的框架。

作者提出了一种既不需要数据增强也不需要负样本的图对比学习方法AFGRL与之前的对比学习方式不同，不是生成两个augmented views进行对比，而是将original graph看作一个view，然后在表征空间中使用KNN选取正样本，无法保证语义不变
过滤假正样本的机制：
1.目标节点在邻接矩阵中的邻居节点（局部视角），捕获了图结构数据中固有的关系归纳偏置。
2.属于目标节点的同一集群（全局视角）
研究背景
基于增强的对比方法的问题：
对于图数据，不合适的数据增强可能会改变图的语义。
使用数据增强后，不能确认新视图是否与原始图正相关。
难以验证有效性，图相对于图像很难可视化。
超参数必须根据数据集和下游任务类型进行调整，不同增强方法的组合对下游任务也高度相关。对模型性能影响较大。在对比学习中，两个样本是同一实例的 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/post/46692965.html" title="MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems"><img class="post_bg" src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems"></a></div><div class="recent-post-info"><a class="article-title" href="/post/46692965.html" title="MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems">MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-13T14:05:54.000Z" title="发表于 2022-11-13 22:05:54">2022-11-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="content">MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems
核心创新

通过合成负样本而不是直接在现有负样本进行采样
提出了positive mixing和hop mixing

研究背景
只聚焦于改善离散图空间中的负采样，忽略了 GNN 在嵌入空间中的邻居聚合过程。
研究方法

image-20221030101126598

positive mixing
将正样本信息注入到 候选集中的负样本嵌入中。对于每个候选的负样本嵌入，positive mixing的过程：

是第层的后悬架负样本嵌入表示，是对每一跳均匀采样的混合系数，符合(0,1)分布.
positive mixing通过 将正样本信息注入负样本中来增强负样本，更好地利用决策边界，以及 使用随机混合系数将随机不确定性引入其中。
hop mixing
具体来说，对于每个layer𝑙（0≤𝑙≤𝐿），从候选集中抽取一个候选负嵌入（1≤𝑥≤𝑀），其中包含所有候选负样本的第� ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/cat.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">wizard</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Wizard's github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="/atom.xml" target="_blank" title="RSS链接"><i class="iconfont icon-rss card_icon"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=710697495&amp;website=www.oicqzone.com" target="_blank" title=""><i class="iconfont icon-QQ card_icon"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">wizard的学习笔记</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/15f17e01.html" title="Knowledge Distillation Meets Self-Supervision"><img src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Knowledge Distillation Meets Self-Supervision"/></a><div class="content"><a class="title" href="/post/15f17e01.html" title="Knowledge Distillation Meets Self-Supervision">Knowledge Distillation Meets Self-Supervision</a><time datetime="2022-12-14T07:49:31.000Z" title="发表于 2022-12-14 15:49:31">2022-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/7709ffe1.html" title="Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework"><img src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework"/></a><div class="content"><a class="title" href="/post/7709ffe1.html" title="Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework">Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework</a><time datetime="2022-12-11T02:48:11.000Z" title="发表于 2022-12-11 10:48:11">2022-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/515ef697.html" title="Selective-Supervised Contrastive Learning with Noisy Labels"><img src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Selective-Supervised Contrastive Learning with Noisy Labels"/></a><div class="content"><a class="title" href="/post/515ef697.html" title="Selective-Supervised Contrastive Learning with Noisy Labels">Selective-Supervised Contrastive Learning with Noisy Labels</a><time datetime="2022-12-02T12:41:18.000Z" title="发表于 2022-12-02 20:41:18">2022-12-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/d81d0a77.html" title="Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning"><img src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning"/></a><div class="content"><a class="title" href="/post/d81d0a77.html" title="Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning">Structural and Semantic Contrastive Learning for Unsupervised Node Representation Learning</a><time datetime="2022-11-27T09:19:51.000Z" title="发表于 2022-11-27 17:19:51">2022-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/ef2c54c1.html" title="Self-supervised Consensus Representation Learning for Attributed Graph"><img src="https://w.wallhaven.cc/full/l3/wallhaven-l37mzp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Self-supervised Consensus Representation Learning for Attributed Graph"/></a><div class="content"><a class="title" href="/post/ef2c54c1.html" title="Self-supervised Consensus Representation Learning for Attributed Graph">Self-supervised Consensus Representation Learning for Attributed Graph</a><time datetime="2022-11-26T00:44:28.000Z" title="发表于 2022-11-26 08:44:28">2022-11-26</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="card-category-list-name">论文阅读</span><span class="card-category-list-count">7</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/GNN/" style="font-size: 1.5em; color: #99a9bf">GNN</a> <a href="/tags/contrastive-learning/" style="font-size: 1.1em; color: #999">contrastive learning</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 1.1em; color: #999">知识蒸馏</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/12/"><span class="card-archive-list-date">十二月 2022</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/11/"><span class="card-archive-list-date">十一月 2022</span><span class="card-archive-list-count">7</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">10</div></div><div class="webinfo-item"><div class="item-name">已运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2022-10-10T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-12-17T09:38:07.020Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/palade.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By wizard</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    window.typed = new Typed("#subtitle", {
      strings: ["脚踏实地，终有所成，终有所爱"],
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("subtitle").innerHTML = '脚踏实地，终有所成，终有所爱'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>